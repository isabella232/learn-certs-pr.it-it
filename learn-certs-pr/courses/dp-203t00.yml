### YamlMime:Course
title: Data Engineering on Microsoft Azure
metadata:
  title: 'Course DP-203T00-A: Data Engineering on Microsoft Azure'
  description: 'Course DP-203T00-A: Data Engineering on Microsoft Azure'
uid: course.dp-203t00
courseNumber: 'DP-203T00-A'
hoursToComplete: 96
iconUrl: /media/learn/certification/course.svg
skillsGained:
- skill: Esplorare opzioni di calcolo e archiviazione per carichi di lavoro data engineering su Azure
- skill: Lanciare query interattive usando pool senza server SQL 
- skill: Eseguire Esplorazione e Trasformazione dei dati su Azure Databricks
- skill: Esplorare, trasformare e caricare dati nella Data Warehouse usando Apache Spark
- skill: Incorporare e caricare Dati sulla Data Warehouse
- skill: Trasformare Dati con Azure Data Factory or Azure Synapse Pipelines
- skill: Integrare Dati da Appunti con Azure Data Factory o Azure Synapse Pipelines
- skill: Supporto Hybrid Transactional Analytical Processing (HTAP) con Azure Synapse Link
- skill: Eseguire sicurezza end-to-end con Azure Synapse Analytics
- skill: Eseguire uno Stream Processing in tempo reale con Stream Analytics
- skill: Creare una Soluzione Stream Processing Solution con Event Hubs e Databrick Azure 
learningPartnersLink: /learn/certifications/partners
locales:
- en
levels:
- intermediate
roles:
- data-engineer
products:
- azure
exams:
- uid: exam.dp-203
summary: |-
  In questo corso gli studenti scopriranno l'ingegneria dati che si occupa di lavorare con soluzioni batch e analisi in tempo reale usando tecnologie della piattaforma dati Azure. Gli studenti inizieranno con una comprensione delle tecnologie centrali di calcolo e archiviazione che vengono usate per costruire una soluzione analitica. Poi impareranno come esplorare in maniera interattiva dati archiviati in file di un data lake. Impareranno le diverse tecniche di assimilazione che possono essere usate per caricare dati usando la potenzialità Apache Spark di Azure Synapse Analytics o Azure Databricks, o come farlo usando pipeline Azure Data Factory o Azure Synapse. Gli studenti impareranno anche i diversi modi per trasformare dati usando le stesse tecnologie che vengono usate per l'assimilazione dati. Capiranno così l'importanza dell'implementazione della sicurezza per assicurarsi che i dati siano protetti sia a riposo che in transito. Gli studenti mostreranno infine come creare un sistema analitico in tempo reale per creare soluzioni analitiche in tempo reale.

  #### Profilo d'utenza
  Il pubblico principale per questo corso è rappresentato da professionisti nel campo dei dati, architetti dati e professionisti nel campo dell'intelligence aziendale che desiderano approfondire il mondo data engineering e costruire soluzioni analitiche usando le tecnologie con piattaforme dati esistenti su Microsoft Azure. Il pubblico secondario per questo corso è costituito da analisti e ricercatori dati che lavorano con soluzioni analitiche sviluppate su Microsoft Azure.
prerequisitesSection: |-
  Gli studenti che avranno successo in questo corso hanno delle conoscenze pregresse nel campo del cloud computing e dei concetti centrali riguardanti i dati, ed esperienza professionale con soluzioni dati.
  
  In particolare, completando&#58;
  
  - AZ-900 - Azure Fundamentals
  - DP-900 - Microsoft Azure Data Fundamentals
outlineSection: |-
  ### Modulo 1&#58; Esplorare opzioni di calcolo e conservazione per carichi di lavoro data engineering 
  Questo modulo offre una panoramica sulle opzioni tecnologiche di calcolo e conservazione di Azure che sono disponibili a data engineers che sviluppano carichi di lavoro analitici. Questo modulo è volto all'insegnamento di metodi per strutturare il data lake e ottimizzare i file per l'esplorazione, lo streaming e carichi di lavoro batch. Gli studenti impareranno come organizzare i data lake in livelli di perfezionamento dati mentre trasformeranno file tramite processi di batch e streaming. Impareranno poi come creare indici sui propri dataset, come file CSV, JSON, e Parquet, e usarli per possibili query o accelerazioni del carico di lavoro. 
  #### Lezioni
  - Introduzione a Azure Synapse Analytics
  - Illustrare Azure Databricks
  - Introduzione alla conservazione su Azure Data Lake 
  - Illustrare l'architettura Delta Lake 
  - Lavorare con stream di dati usando Azure Stream Analytics

  #### Lab &#58; Esplorare opzioni di calcolo e conservazione per carichi di lavoro data engineering 
  *   Combinare processi streaming e di elaborazione con una sola pipeline
  *   Organizzare il data lake in livelli di trasformazione file 
  *   Indicizzare l'archiviazione del data lake per un'accelerazione di ricerche e carichi di lavoro 
  
  Dopo aver completato questo modulo, gli studenti saranno in grado di&#58;
  - Illustrare Azure Synapse Analytics
  - Illustrare Azure Databricks
  - Illustrare la conservazione su Azure Data Lake 
  - Illustrare l'architettura Delta Lake 
  - Illustrare Azure Stream Analytics
  
  ### Modulo 2&#58; Eseguire query interattive usando pool serverless SQL su Azure Synapse Analytics 
  In questo modulo gli studenti impareranno come lavorare con file archiviati in un data lake e sorgenti file esterne, tramite istruzioni T-SQL eseguite da un pool serverless SQL su Azure Synapse Analytics. Gli studenti cercheranno file Parquet archiviati in un data lake, e anche file CSV archiviati in un archivio dati esterno. Poi, creeranno gruppi di sicurezza su Azure Active Directory e rafforzeranno l'accesso ai file nel data lake tramite il Role-Based Access Control (RBAC) e le Access Control Lists (ACLs).
  #### Lezioni
  - Esplorare le potenzialità dei pool serverless SQL su Azure Synapse
  - Cercare dati nel lake usando pool serverless SQL usando Azure 
  - Creare oggetti metadati su pool serverless SQL su Azure Synapse 
  - Rendere sicuri dati e gestire utenti su pools serverless SQL su Azure Synapse 
  
  #### Lab &#58; Lanciare query interattive usando pool serverless SQL 
  - Cercare dati Parquet con pool serverless SQL 
  - Creare tabelle esterne per file Parquet e CSV 
  - Creare viste con pool serverless SQL 
  - Rendere sicuri i dati in un data lake usando i pool serverless SQL 
  - Configurare la sicurezza del data lake usando il Role-Based Access Control (RBAC) e l'Access Control List
  
  Dopo aver completato questo modulo, gli studenti saranno in grado di&#58;
  - Capire le potenzialità dei pool serverless SQL su Azure Synapse
  - Cercare dati nel lake usando pool serverless SQL usando Azure 
  - Creare oggetti metadati su pool serverless SQL su Azure Synapse 
  - Rendere sicuri dati e gestire utenti su pool serverless SQL su Azure Synapse 
  
  ### Modulo 3: Esplorazione e trasformazione dati su Azure Databricks

  Questo modulo insegna come usare diversi metodi di Apache Spark DataFrame per esplorare e trasformare dati su Azure Databricks. Lo studente imparerà come eseguire metodi standard DataFrame per esplorare e trasformare dati. Imparerà anche come eseguire attività più avanzate, come rimuovere dati duplicati, manipolare valori data/ora, rinominare colonne e aggregare dati.

  #### Lezioni

  *   Illustrare Azure Databricks

  *   Leggere e scrivere dati su Azure Databricks

  *   Lavorare con DataFrames su Azure Databricks

  *   Lavorare con metodi avanzati DataFrames su Azure Databricks


  #### Lab : Esplorazione e Trasformazione Dati su Azure Databricks

  ####
  *   Usare DataFrames su Azure Databricks per esplorare e filtrare dati 
  *   Memorizzare un DataFrame per ricerche successive più rapide 
  *   Rimuovere dati duplicati 
  *   Manipolare valori data/ora 
  *   Rimuovere e rinominare colonne DataFrame 
  *   Aggregare dati archiviati in un DataFrame

  Dopo aver completato questo modulo, gli studenti saranno in grado di:

  *   Illustrare Azure Databricks

  *   Leggere e scrivere dati su Azure Databricks

  *   Lavorare con DataFrames su Azure Databricks

  *   Lavorare con metodi avanzati DataFrames su Azure Databricks


  ### Modulo 4: Esplorare, trasformare e caricare dati nella Data Warehouse usando Apache Spark

  Questo modulo insegna come esplorare dati archiviati in un data lake, come trasformare i dati e caricarli in un archivio dati relazionale. Lo studente esplorerà file Parquet e JSON e userà tecniche per cercare e trasformare i file JSON con strutture gerarchiche. Poi lo studente userà Apache Spark per caricare dati nella data warehouse e unirà dati Parquet nel data lake con dati nel pool SQL dedicato.

  #### Lezioni

  *   Capire il big data engineering con Apache Spark su Azure Synapse Analytics

  *   Assimilare dati con quaderni Apache Spark su Azure Synapse Analytics

  *   Trasformare dati con DataFrames in Pool Apache Spark su Azure Synapse Analytics

  *   Integrare pool SQL e Apache Spark su Azure Synapse Analytics


  #### Lab : Esplorare, trasformare e caricare dati nella Data Warehouse usando Apache Spark

  ####
  *   Eseguire un'Esplorazione Dati su Synapse Studio
  *   Assimilare dati con quaderni Spark su Azure Synapse Analytics
  *   Trasformare dati con DataFrames in Pool Spark su Azure Synapse Analytics
  *   Integrare pool SQL e Spark su Azure Synapse Analytics

  Dopo aver completato questo modulo, gli studenti saranno in grado di:

  *   Capire il big data engineering con Apache Spark su Azure Synapse Analytics

  *   Assimilare dati con quaderni Apache Spark su Azure Synapse Analytics

  *   Trasformare dati con DataFrames in Pool Apache Spark su Azure Synapse Analytics

  *   Integrare pool SQL e Apache Spark su Azure Synapse Analytics
  
  ### Modulo 5: Assimilare e caricare dati nella data warehouse

  Questo modulo insegna agli studenti come assimilare dati nella data warehouse tramite script T-SQL e pipeline di integrazione Synapse Analytics. Lo studente imparerà come caricare dati nel pool SQL dedicato Synapse con PolyBase e COPY usando T-SQL. Lo studente imparerà anche come usare la gestione dei carichi di lavoro insieme ad un'attività Copia in una pipeline Azure Synapse per un'assimilazione dati in scala petabyte.

  #### Lezioni

  *   Usare le migliori pratiche di caricamento dati su Azure Synapse Analytics

  *   Assimilazione in scala petabyte con Azure Data Factory


  #### Lab : Assimilare e Caricare dati nella Data Warehouse

  ####
  *   Eseguire un'assimilazione in scala petabyte con Azure Synapse Pipelines
  *   Importare dati con PolyBase e COPY usando T-SQL
  *   Usare le migliori pratiche di caricamento dati su Azure Synapse Analytics

  Dopo aver completato questo modulo, gli studenti saranno in grado di:

  *   Usare le migliori pratiche di caricamento dati su Azure Synapse Analytics

  *   Assimilazione in scala petabyte con Azure Data Factory


  ### Modulo 6: Trasformare dati con Azure Data Factory o Azure Synapse Pipelines

  Questo modulo insegna agli studenti come costruire pipeline di integrazione dati per assimilarli da diverse fonti dati, come trasformare i dati usando flussi di dati mappati, ed eseguire un movimento dati in uno più bacini dati.

  #### Lezioni

  *   Integrazione dati con Azure Data Factory o Azure Synapse Pipelines

  *   Trasformazione priva di codici in scala con Azure Data Factory o Azure Synapse Pipelines


  #### Lab : Trasformare dati con Azure Data Factory o Azure Synapse Pipelines

  ####
  *   Eseguire trasformazioni prive di codici in scala con Azure Synapse Pipelines
  *   Creare pipeline di dati per importare file CSV di scarsa formattazione 
  *   Creare Flussi di Dati Mappati 

  Dopo aver completato questo modulo, gli studenti saranno in grado di:
  *   Eseguire un'integrazione dati con Azure Data Factory

  *   Eseguire una trasformazione priva di codici in scala con Azure Data Factory 
  
  ### Modulo 7: Organizzare movimento e trasformazione dati su Azure Synapse Pipelines

  In questo modulo imparerai come creare servizi collegati e come organizzare il movimento e la trasformazione dei dati usando quaderni su Azure Synapse Pipelines.

  #### Lezioni

  *   Organizzare movimento e trasformazione dati su Azure Data Factory


  #### Lab : Organizzare movimento e trasformazione dati su Azure Synapse Pipelines

  ####
  *   Integrare Dati da Quaderni con Azure Data Factory o Azure Synapse Pipelines

  Dopo aver completato questo modulo, gli studenti saranno in grado di:

  *   Organizzare movimento e trasformazione dati su Azure Synapse Pipelines


  ### Modulo 8: Sicurezza end-to-end con Azure Synapse Analytics

  In questo modulo gli studenti impareranno come rendere sicuri uno spazio di lavoro Synapse Analytics e la sua infrastruttura di supporto. Lo studente osserverà il SQL Active Directory Admin, gestirà regole di firewall IP, gestirà informazioni private con Azure Key Vault e vi accederà tramite un servizio collegato e attività di pipeline Key Vault. Lo studente capirà come implementare la sicurezza a livello di colonne, la sicurezza a livello di riga, e il data masking dynamic durante l'uso di pool dedicati SQL.

  #### Lezioni

  *   Rendere sicura una data warehouse su Azure Synapse Analytics

  *   Configurare e gestire informazioni segrete su Azure Key Vault

  *   Implementare i controlli di conformità per dati sensibili 


  #### Lab : Sicurezza end-to-end con Azure Synapse Analytics

  ####
  *   Rendere sicura l'infrastruttura di supporto di Azure Synapse Analytics 
  *   Rendere sicuri lo spazio di lavoro e i servizi gestiti di Azure Synapse Analytics 
  *   Rendere sicuri i dati dello spazio di lavoro Azure Synapse Analytics 

  Dopo aver completato questo modulo, gli studenti saranno in grado di:

  *   Rendere sicura una data warehouse su Azure Synapse Analytics

  *   Configurare e gestire informazioni segrete su Azure Key Vault

  *   Implementare i controlli di conformità per dati sensibili 
  
  ### Modulo 9: Supportare Hybrid Transactional Analytical Processing (HTAP) con Azure Synapse Link

  In questo modulo gli studenti impareranno come Azure Synapse Link consente una connettività ottimale di un account Azure Cosmos DB con uno spazio di lavoro Synapse. Lo studente capirà come abilitare e configurare collegamenti Synapse, quindi come cercare nell'archivio analitico Azure Cosmos DB usando Apache Spark e SQL senza server.

  #### Lezioni

  *   Progettare un'elaborazione analitica e transazionale ibrida usando Azure Synapse Analytics

  *   Configurare Azure Synapse Link con Azure Cosmos DB

  *   Fare ricerche in Azure Cosmos DB con pool Apache Spark 

  *   Fare ricerche in Azure Cosmos DB con pool senza server SQL 


  #### Lab : Supportare Hybrid Transactional Analytical Processing (HTAP) con Azure Synapse Link

  ####
  *   Configurare Azure Synapse Link con Azure Cosmos DB
  *   Fare ricerche in Azure Cosmos DB con Apache Spark per Synapse Analytics
  *   Fare ricerche in Azure Cosmos DB con pool senza server SQL per Azure Synapse Analytics

  Dopo aver completato questo modulo, gli studenti saranno in grado di:
  ​*   Progettare un'elaborazione analitica e transazionale ibrida usando Azure Synapse Analytics

  ​*   Configurare Azure Synapse Link con Azure Cosmos DB

  ​*   Fare ricerche in Azure Cosmos DB con pool Apache Spark 

  ​*   Fare ricerche in Azure Cosmos DB con pool senza server SQL


  ### Modulo 10: Elaborazione del Flusso in Tempo Reale con Stream Analytics

  In questo modulo gli studenti impareranno come elaborare flussi di dati con Azure Stream Analytics. Lo studente assimilerà dati telemetrici vettore in Event Hubs, poi li elaborerà in tempo reale usando varie funzioni di windowing su Azure Stream Analytics. Esporterà quindi i dati in Azure Synapse Analytics. Infine, lo studente imparerà come ridimensionare l'attività Stream Analytics per promuovere la produttività.

  #### Lezioni

  *   Abilitare messaggi affidabili per applicazioni Big Data usando Azure Event Hubs

  *   Lavorare con flussi di dati usando Azure Stream Analytics

  *   Assimilare flussi di dati Azure Stream Analytics


  #### Lab : Elaborazione del Flusso in Tempo Reale con Stream Analytics

  ####
  *   Usare Stream Analytics per elaborare dati in tempo reale da Event Hubs
  *   Usare funzioni windowing di Stream Analytics per costruire aggregati ed esportarli su Synapse Analytics
  *   Ridimensionare l'attività Azure Stream Analytics per aumentare la produttività tramite porzionamento 
  *   Ripartire l'input del flusso per ottimizzare la parallelizzazione 

  Dopo aver completato questo modulo, gli studenti saranno in grado di:

  *   Abilitare messaggi affidabili per applicazioni Big Data usando Azure Event Hubs

  *   Lavorare con flussi di dati usando Azure Stream Analytics

  *   Assimilare flussi di dati Azure Stream Analytics

  ### Modulo 11: Creare una Soluzione di Elaborazione Flusso con Event Hubs e Azure Databricks

  In questo modulo gli studenti impareranno come assimilare ed elaborare flussi di dati in scala con Event Hubs e Spark Structured Streaming su Azure Databricks. Lo studente imparerà le funzionalità chiave e gli usi di Structured Streaming. Poi implementerà le finestre scorrevoli per un'aggregazione di chunk di dati e applicherà il watermarking per rimuovere i dati obsoleti. Infine, lo studente si connetterà a Event Hubs per leggere e scrivere flussi.

  #### Lezioni

  *   Elaborare flussi di dati con Azure Databricks structured streaming


  #### Lab : Creare una Soluzione di Elaborazione Flusso con Event Hubs e Azure Databricks

  ####
  *   Esplorare le funzionalità chiave e gli usi di Structured Streaming
  *   Riprodurre dati da un file e scriverli in un sistema di file distribuiti 
  *   Usare finestre scorrevole per aggregare chunk di dati invece che tutti i dati
  *   Applicare watermarking per rimuovere dati obsoleti
  *   Connettersi a Event Hubs per leggere e scrivere flussi

  Dopo aver completato questo modulo, gli studenti saranno in grado di:

  *   Elaborare flussi di dati con Azure Databricks structured streaming
